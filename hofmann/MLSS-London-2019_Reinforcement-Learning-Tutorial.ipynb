{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Summer School - London 2019\n",
    "## Reinforcement Learning Tutorial\n",
    "\n",
    "**Student version with exercises**\n",
    "\n",
    "### Author: [Katja Hofmann](https://www.microsoft.com/en-us/research/people/kahofman/)\n",
    "\n",
    "This tutorial uses the [MineRL package](http://minerl.io/) to illustrate how a Reinforcement Learning (RL) agent can learn to interact with the popular video game [Minecraft](https://www.minecraft.net/en-us/). MineRL was developed by a team led by [William H. Guss](http://wguss.ml/) and [Brandon Houghton](https://github.com/brandonhoughton) for the NeurIPS 2019 MineRL competition, hosted by AICrowd and sponsored by Microsoft. MineRL is based on [Project Malmo](https://www.microsoft.com/en-us/research/project/project-malmo/), developed at [Microsoft Research](https://www.microsoft.com/en-us/research/theme/game-intelligence/). \n",
    "\n",
    "This tutorial uses the deep learning framework [chainer](https://chainer.org/) to implement RL algorithms. The tutorial is designed to use as few chainer-specific constructs, but if you'd like to learn more about chainer, take a look at the [documentation](https://docs.chainer.org/en/stable/glance.html). Chainer is developed by [Preferred Networks (PFN)](https://www.preferred-networks.jp/en/), a co-organizer of the MineRL competition. The code here is designed for educational purposes. If you'd like to explore reinforcement learning and the MineRL competition further, take a look at the [chainerrl minerl baselines](https://github.com/minerllabs/quick_start/tree/master/chainerrl_baselines) provided by PFN.\n",
    "\n",
    "### Overview\n",
    "\n",
    "This tutorial demonstrates how to build an RL agent that learns to navigate: first in SimpleRooms, a task we implement from scratch, then in a MineRL task in Minecraft. Some parts of the tutorial are optional, as shown below - you can learn about key concepts in reinforcement learning and implement your first agent without installing the MineRL package.\n",
    "\n",
    "1. [Setup](#Setup) **Tip: run this section before the start of the tutorial, to make sure you're ready to get started.**\n",
    "  1. [General Prerequisites](#General-Prerequisites)\n",
    "  1. [Install the MineRL Package](#Install-the-MineRL-Package) (optional): install MineRL. If skipped, you can still follow the agent implementation and run it on the simple maze environment.\n",
    "  1. [Test MineRL](#Test-MineRL) (optional): start Minecraft and test out interaction with the game.\n",
    "\n",
    "1. [RL Components](#RL-Components): **Learn how to implement the core components of an RL experiment: environment, agent, and the experiment itself.**\n",
    "  1. [Environment](#Environment)\n",
    "  1. [Agent](#Agent)\n",
    "  1. [Experiment](#Experiment)\n",
    "  1. [Experiment 1: Random Agent on SimpleRooms](#Experiment-1:-Random-Agent-on-SimpleRooms)\n",
    "  1. [Experiment 2: Random Agent on MineRL-NavDense](#Experiment-2:-Random-Agent-on-MineRL-NavDense) (optional)\n",
    "\n",
    "1. [DQN Agent Implementation](#DQN-Agent-Implementation): **Learn how to implement a DQN Agent**\n",
    "  1. [QLearning Agent](#QLearning-Agent)\n",
    "  1. [Exploration](#Exploration)\n",
    "  1. [QNetwork](#QNetwork)\n",
    "  1. [Model Update](#Model-Update)\n",
    "  1. [Optimizer](#Optimizer)\n",
    "  1. [Replay Memory](#Replay-Memory)\n",
    "  1. [DQN Exercises](#DQN-Exercises)\n",
    "\n",
    "1. [Experiment 3: DQN on SimpleRooms](#Experiment-3:-DQN-on-SimpleRooms): **Experiment with a DQN Agent**\n",
    "1. [Experiment 4: DQN on MineRL Navigation](#Experiment-4:-DQN-on-MineRL-Navigation) (optional)\n",
    "1. [Further Reading](#Further-Reading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install required packages and ensure imports work as expected. I recommend you run this section before the start of the tutorial, so that you're all ready to go from the start.\n",
    "\n",
    "### General Prerequisites\n",
    "\n",
    "Install and import packages that are required throughout the tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# install required packages\n",
    "!pip install --upgrade chainer opencv-python gym matplotlib==3.0.3 numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environments\n",
    "import gym\n",
    "\n",
    "# chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import initializers\n",
    "from chainer import serializers\n",
    "from chainer import optimizers, Chain, Variable\n",
    "\n",
    "# visualization\n",
    "%matplotlib nbagg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as anim\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import pylab\n",
    "from IPython import display\n",
    "\n",
    "# utilities\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the MineRL Package\n",
    "\n",
    "\n",
    "If this is the first time you run the tutorial, install the latest MineRL package as shown below. \n",
    "For additional details and prerequisites, see http://minerl.io/docs/tutorials/getting_started.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# uncomment to install minerl\n",
    "!pip install --upgrade minerl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the minerl package. If needed for troubleshooting, this is a good place to enable debug level logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environments\n",
    "import minerl\n",
    "\n",
    "# Get DEBUG logging from MineRL while Minecraft starts up\n",
    "# Uncomment in case Minecraft fails to start, to help with debugging\n",
    "#import sys\n",
    "#import logging\n",
    "#logger = logging.getLogger(\"minerl\")\n",
    "#logger.setLevel(logging.DEBUG)\n",
    "#logger.addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test MineRL\n",
    "\n",
    "Test your MineRL installation by instantiating an environment, and take a first few steps in this Minecraft navigation environment. The example below is based on the [MineRL Tutorial](http://minerl.io/docs/tutorials/first_agent.html).\n",
    "\n",
    "First, we instantiate a MineRL gym environment. This will take a couple of minutes, as Minecraft is started in the background. \n",
    "\n",
    "If something goes wrong, take a look at the previous cell to enable DEBUG logging. If enabled, this generates detailed logs to help with debugging. If something goes wrong, check with your tutorial instructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start Minecraft and create a MineRL environment - be patient, this will take several minutes\n",
    "nav_env = gym.make('MineRLNavigateDense-v0')\n",
    "\n",
    "# If you turned on debugging above, quieten things down by uncommenting the below:\n",
    "#logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready for our first interaction with the environment. Initially, actions are hard coded to move towards the direction indicated by the compass, as shown in the [MineRL Tutorial](http://minerl.io/docs/tutorials/first_agent.html). The code below also visualizes the observations and rewards received from the environment.\n",
    "\n",
    "A detailed specification of the MineRLNavigateDense-v0 environment is available here: http://minerl.io/docs/environments/index.html#minerlnavigatedense-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test interaction with the environment\n",
    "\n",
    "obs, _ = nav_env.reset() # this may take up to a minute, as Minecraft resets\n",
    "done = False\n",
    "\n",
    "# prepare visuals\n",
    "fig = pylab.figure(figsize=(9, 5))\n",
    "gs = gridspec.GridSpec(1, 2)\n",
    "ax1 = pylab.subplot(gs[0, 0])\n",
    "ax1.xaxis.set_visible(False)\n",
    "ax1.yaxis.set_visible(False)\n",
    "\n",
    "# initialize the left plot with the first visual frame received from the environment\n",
    "imgplot = ax1.imshow(obs['pov'])\n",
    "ax2 = pylab.subplot(gs[0, 1])\n",
    "\n",
    "# initialize the right plot with the rewards obtained from the environemt so far\n",
    "rewards = [0]\n",
    "line, = ax2.plot(range(len(rewards)), rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set timer and counts\n",
    "from time import time\n",
    "start = time()\n",
    "net_reward = 0\n",
    "stepcount = 0\n",
    "maxsteps = 250\n",
    "\n",
    "print(\"Note the visualization generated above ^^\")\n",
    "\n",
    "# interact with the environment, results are updated in the plot above\n",
    "while not done:\n",
    "    action = nav_env.action_space.noop()\n",
    "    action['camera'] = [0, 0.03*obs['compassAngle']]\n",
    "    action['back'] = 0\n",
    "    action['forward'] = 1\n",
    "    action['jump'] = 1\n",
    "    action['attack'] = 1\n",
    "\n",
    "    obs, reward, done, info = nav_env.step(action)\n",
    "    rewards.append(reward)\n",
    "\n",
    "    if stepcount % 5 == 0:\n",
    "        imgplot.set_data(obs['pov'])\n",
    "        line.set_data(range(len(rewards)), rewards)\n",
    "        ax2.set_xlim(0, len(rewards))\n",
    "        ax2.set_ylim(min(rewards), max(rewards))\n",
    "        fig.canvas.draw()\n",
    "\n",
    "    stepcount += 1\n",
    "    if stepcount >= maxsteps:\n",
    "        break\n",
    "\n",
    "print(\"Time taken for %d steps: %.1f seconds\" % (maxsteps, time() - start))\n",
    "print(\"Total reward: %.1f\" % sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the last action taken\n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've succesfully installed all prerequisites, and you have taken your first steps in the Minecraft world. \n",
    "\n",
    "**Exercise 1:** Take a moment to go through the code above and understand the components. Discuss with your neighbor to answer the following question: Where are the components of the environment interaction implemented: receiving observations and rewards, and sending actions to the environment?\n",
    "\n",
    "**Exercise 2:** Try manipulating the actions sent to the environment. Can you make the Minecraft agent run in a circle? Can you make it mine a block?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Components\n",
    "\n",
    "In this section, we set up a number of components that make it easy to run and visualize RL experiments in Jupyter Notebooks. The main components are:\n",
    "\n",
    "**Environment:** defines an interactive task. We assume environments implement the [OpenAI gym interface](https://gym.openai.com/). In addition to the MineRL environment introduced above, we will also implement a toy task, SimpleRooms, to illustrate typical environment functionality.\n",
    "\n",
    "**Agent:** interacts with an environment by receiving observations and rewards, and taking actions. We will implement several agents throughout this tutorial, starting from a random agent and moving to a Deep Q-Network agent that implements Q-Learning.\n",
    "\n",
    "**Experiment:** connects agents and environments, collects and reports results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment interface - defines how the experiment interacts with the environment\n",
    "\n",
    "class Environment(object):\n",
    "\n",
    "    def reset(self):\n",
    "        raise NotImplementedError('Inheriting classes must override reset.')\n",
    "\n",
    "    def actions(self):\n",
    "        raise NotImplementedError('Inheriting classes must override actions.')\n",
    "\n",
    "    def step(self):\n",
    "        raise NotImplementedError('Inheriting classes must override step')\n",
    "\n",
    "class ActionSpace(object):\n",
    "    \n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "        self.n = len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRooms(Environment):\n",
    "    \"\"\"Define a simple 4-room environment with 16 states\n",
    "       actions: 0 - north, 1 - east, 2 - west, 3 - south\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SimpleRooms, self).__init__()\n",
    "\n",
    "        # define state and action space\n",
    "        self.S = range(16)\n",
    "        self.action_space = ActionSpace(range(4))\n",
    "\n",
    "        # define reward structure\n",
    "        self.R = [0] * len(self.S)\n",
    "        self.R[random.choice(self.S)] = 1\n",
    "\n",
    "        # define transitions\n",
    "        self.P = {}\n",
    "        self.P[0] = [1, 4]\n",
    "        self.P[1] = [0, 2, 5]\n",
    "        self.P[2] = [1, 3, 6]\n",
    "        self.P[3] = [2, 7]\n",
    "        self.P[4] = [0, 5, 8]\n",
    "        self.P[5] = [1, 4]\n",
    "        self.P[6] = [2, 7]\n",
    "        self.P[7] = [3, 6, 11]\n",
    "        self.P[8] = [4, 9, 12]\n",
    "        self.P[9] = [8, 13]\n",
    "        self.P[10] = [11, 14]\n",
    "        self.P[11] = [7, 10, 15]\n",
    "        self.P[12] = [8, 13]\n",
    "        self.P[13] = [9, 12, 14]\n",
    "        self.P[14] = [10, 13, 15]\n",
    "        self.P[15] = [11, 14]\n",
    "\n",
    "        self.max_trajectory_length = 50\n",
    "        self.tolerance = 0.1\n",
    "        self._rendered_maze = self._render_maze()\n",
    "\n",
    "    def step(self, action):\n",
    "        s_prev = self.s\n",
    "        self.s = self.single_step(self.s, action)\n",
    "        reward = self.single_reward(self.s, s_prev, self.R)\n",
    "        self.nstep += 1\n",
    "        self.is_reset = False\n",
    "\n",
    "        if (reward < -1. * (self.tolerance) or reward > self.tolerance) or self.nstep == self.max_trajectory_length:\n",
    "            self.reset()\n",
    "\n",
    "        return (self._convert_state(self.s), reward, self.is_reset, '')\n",
    "\n",
    "    def single_step(self, s, a):\n",
    "        if a < 0 or a > 3:\n",
    "            raise ValueError('Unknown action', a)\n",
    "        if a == 0 and (s-4 in self.P[s]):\n",
    "            s -= 4\n",
    "        elif a == 1 and (s+1 in self.P[s]):\n",
    "            s += 1\n",
    "        elif a == 2 and (s-1 in self.P[s]):\n",
    "            s -= 1\n",
    "        elif a == 3 and (s+4 in self.P[s]):\n",
    "            s += 4\n",
    "        return s\n",
    "\n",
    "    def single_reward(self, s, s_prev, rewards):\n",
    "        if s == s_prev:\n",
    "            return 0\n",
    "        return rewards[s]\n",
    "\n",
    "    def reset(self):\n",
    "        self.nstep = 0\n",
    "        self.s = random.choice(self.S)\n",
    "        # disallow spawning in a reward state\n",
    "        while (self.R[self.s] < -1. * (self.tolerance) or self.R[self.s] > self.tolerance):\n",
    "            self.s = random.choice(self.S)\n",
    "        self.is_reset = True\n",
    "        return self._convert_state(self.s)\n",
    "\n",
    "    def _convert_state(self, s):\n",
    "        converted = np.zeros(len(self.S), dtype=np.float32)\n",
    "        converted[s] = 1\n",
    "        return converted\n",
    "\n",
    "    def _get_render_coords(self, s):\n",
    "        return (int(s / 4) * 4, (s % 4) * 4)\n",
    "\n",
    "    def _render_maze(self):\n",
    "        # draw background and grid lines\n",
    "        maze = np.zeros((17, 17))\n",
    "        for x in range(0, 17, 4):\n",
    "            maze[x, :] = .2\n",
    "        for y in range(0, 17, 4):\n",
    "            maze[:, y] = .2\n",
    "\n",
    "        # draw reward and transitions\n",
    "        for s in range(16):\n",
    "            if self.R[s] != 0:\n",
    "                x, y = self._get_render_coords(s)\n",
    "                maze[x+1:x+4, y+1:y+4] = self.R[s]\n",
    "            if self.single_step(s, 0) == s:\n",
    "                x, y = self._get_render_coords(s)\n",
    "                maze[x, y:y+5] = .5\n",
    "            if self.single_step(s, 1) == s:\n",
    "                x, y = self._get_render_coords(s)\n",
    "                maze[x:x+5, y+4] = .5\n",
    "            if self.single_step(s, 2) == s:\n",
    "                x, y = self._get_render_coords(s)\n",
    "                maze[x:x+5, y] = .5\n",
    "            if self.single_step(s, 3) == s:\n",
    "                x, y = self._get_render_coords(s)\n",
    "                maze[x+4, y:y+4] = .5\n",
    "        return maze\n",
    "\n",
    "    def render(self, mode = 'rgb_array'):\n",
    "        assert mode == 'rgb_array', 'Unknown mode: %s' % mode\n",
    "        img = np.array(self._rendered_maze, copy=True)\n",
    "\n",
    "        # draw current agent location\n",
    "        x, y = self._get_render_coords(self.s)\n",
    "        img[x+1:x+4, y+1:y+4] = .8\n",
    "\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    '''Agent base class'''\n",
    "\n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "        self.num_actions = len(actions)\n",
    "\n",
    "    def step(self, obs, reward, done, info):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "    '''Agent that samples actions uniformly at random'''\n",
    "\n",
    "    def __init__(self, actions):\n",
    "        super(RandomAgent, self).__init__(actions)\n",
    "    \n",
    "    def step(self, obs, reward, done, info):\n",
    "        self.current_loss = 0\n",
    "        return random.randint(0, self.num_actions-1)\n",
    "\n",
    "class DefaultAgent(Agent):\n",
    "    '''Agent that always takes a default action'''\n",
    "    def __init__(self, actions, default_action):\n",
    "        super(DefaultAgent, self).__init__(actions)\n",
    "        self.default_action = default_action\n",
    "    \n",
    "    def step(self, obs, reward, done, info):\n",
    "        self.current_loss = 0\n",
    "        return self.default_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment(object):\n",
    "\n",
    "    def __init__(self, env, agent, normobs=False):\n",
    "        \n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "\n",
    "        self.epoch_losses = [0]\n",
    "        self.rolling_average = np.array([0])\n",
    "        self.windowsize = 100\n",
    "        self.normalize_observations = normobs\n",
    "\n",
    "        # prepare visuals\n",
    "        self.fig = pylab.figure(figsize=(9, 5))\n",
    "        gs = gridspec.GridSpec(2, 2)\n",
    "        self.ax = pylab.subplot(gs[:, 0])\n",
    "        self.ax.title.set_text('Current frame')\n",
    "        self.ax.xaxis.set_visible(False)\n",
    "        self.ax.yaxis.set_visible(False)\n",
    "        self.ax1 = pylab.subplot(gs[0, 1])\n",
    "        self.ax1.title.set_text('Rolling average reward')\n",
    "        self.ax2 = pylab.subplot(gs[1, 1])\n",
    "        self.ax2.title.set_text('Average loss')\n",
    "        \n",
    "        self.line, = self.ax1.plot(range(len(self.rolling_average)), self.rolling_average)\n",
    "        self.line2, = self.ax2.plot(range(len(self.epoch_losses)), self.epoch_losses)\n",
    "        self.imgplot = self.ax.imshow(np.random.random((64,64)), interpolation='none', cmap='viridis')\n",
    "        self.first_render = True\n",
    "\n",
    "        pylab.show()\n",
    "\n",
    "    def run(self, num_steps, display_frequency):\n",
    "        self.display_frequency = display_frequency\n",
    "        observation = self.env.reset()\n",
    "        self.update_display()\n",
    "        steps = 0\n",
    "        done = False\n",
    "        reward = .0\n",
    "        rewards = np.array([])\n",
    "        losses = []\n",
    "\n",
    "        while steps < num_steps:\n",
    "            steps += 1\n",
    "            if self.normalize_observations:\n",
    "                observation = (observation / 255.).astype(np.float32, copy=False)\n",
    "            action = self.agent.step(observation, reward, done, None)\n",
    "            observation, reward, done, _ = self.env.step(action)\n",
    "            losses.append(self.agent.current_loss)\n",
    "\n",
    "            if done:\n",
    "                observation = self.env.reset()\n",
    "\n",
    "            rewards = np.append(rewards, reward)\n",
    "            self.rolling_average = np.append(self.rolling_average,\n",
    "                                        sum(rewards[-self.windowsize:])/len(rewards[-self.windowsize:]))\n",
    "\n",
    "            if steps % self.display_frequency == 0:\n",
    "                self.epoch_losses = np.append(self.epoch_losses, np.mean(losses))\n",
    "                self.update_display()\n",
    "                losses = []\n",
    "      \n",
    "    def update_display(self):\n",
    "        self.imgplot.set_data(self.env.render(mode='rgb_array'))\n",
    "        self.line.set_data(range(len(self.rolling_average)), self.rolling_average)\n",
    "        self.ax1.set_xlim(0, max(100, len(self.rolling_average)))\n",
    "        self.ax1.set_ylim(min(self.rolling_average)-0.01, max(self.rolling_average)+0.01 * 1.1)\n",
    "\n",
    "        self.line2.set_data(range(0, len(self.epoch_losses)), self.epoch_losses)\n",
    "        self.ax2.set_xlim(0, max(100, len(self.epoch_losses)))\n",
    "        self.ax2.set_ylim(min(min(self.epoch_losses), 1e-5), max(self.epoch_losses)+0.01 * 1.1)\n",
    "        self.fig.canvas.draw()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Random Agent on SimpleRooms\n",
    "\n",
    "We are ready to set up a first simple experiment. This illustrates how an experiment connects environment and agent. We'll run a random agent on the SimpleRooms environment. The first cell below instantiates the components of the RL experiment and initializes the visualization. The cell below it runs the actual experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# experiment setup\n",
    "simple_env = SimpleRooms()\n",
    "random_agent = RandomAgent(simple_env.action_space.actions)\n",
    "experiment = Experiment(simple_env, random_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the experiment for 200 steps, and visualize every 1 step\n",
    "experiment.run(200, 1)\n",
    "# re-run this cell as often as you like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now assembled your first RL experiment. Take a step back to familiarize yourself with the code up to this point.\n",
    "\n",
    "**Exercise 3:** Consider the implementation of the SimpleRooms environment. This implements a Markov Decision Process (MDP). Can you identify the key components of the MDP: state and action space, transition and reward function? Modify the SimpleRooms environment to change transition and reward function, e.g., create two connected rooms, and fix the goal to a constant location.\n",
    "\n",
    "**Exercise 4:** Now take a look at the simple agents implemented above. So far, we used the RandomAgent. Set up an experiment with the DefaultAgent. Then, modify the DefaultAgent to solve a task with a fized goal location.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Random Agent on MineRL-NavDense\n",
    "\n",
    "Now for the real thing - our first experiment with the MineRL environment. We will simplify the environment, for illustration and to make learning the task within a couple of minutes feasible. We'll simplify the action and observation space, as well as providing a simpler reward signal, as implemented in the environment wrapper below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteMinecraftEnvWrapper(Environment):\n",
    "    '''Wrap a MineRL environment to discretize actions - assume Nav environemnt'''\n",
    "\n",
    "    def __init__(self, env):\n",
    "\n",
    "        self.env = env\n",
    "        # define action space\n",
    "        self.action_space = ActionSpace(range(3))\n",
    "\n",
    "    def reset(self):\n",
    "        self.obs, _ = self.env.reset()\n",
    "        self.steps_this_episode = 0\n",
    "        return self._convert_obs(self.obs)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps_this_episode += 1\n",
    "        self.obs, self.reward, self.done, self.info = self.env.step(self._convert_action(action))\n",
    "        # simplify reward signal\n",
    "        if action == 0:\n",
    "            if obs['compassAngle'] < 1:\n",
    "                self.reward = .5\n",
    "            else:\n",
    "                self.reward = .1\n",
    "        else:\n",
    "            self.reward = -.3\n",
    "        return self._convert_obs(self.obs), self.reward, self.done, self.info\n",
    "\n",
    "    def _convert_obs(self, obs):\n",
    "        '''Extract visuals'''\n",
    "        # constructs obs of size 3 x 3 x 3 + 1 = 28\n",
    "        low_res = cv2.resize(obs['pov'], dsize=(3, 3), interpolation=cv2.INTER_NEAREST)\n",
    "        return np.float32(np.hstack([low_res.flatten(), obs['compassAngle']]))\n",
    "\n",
    "    def _convert_action(self, action):\n",
    "        base_action =  self.env.action_space.noop()\n",
    "        base_action['jump'] = 1\n",
    "        base_action['attack'] = 1\n",
    "\n",
    "        if action == 0:\n",
    "            # move forward\n",
    "            base_action['forward'] = 1\n",
    "        elif action == 1:\n",
    "            # turn towards the compass direction\n",
    "            base_action['camera'] = [0, 0.03 * obs['compassAngle']]\n",
    "        elif action == 2:\n",
    "            # move back\n",
    "            base_action['back'] = 1\n",
    "        else:\n",
    "            raise NotImplementedError('Action %d is not implemented.' % action)\n",
    "\n",
    "        return base_action\n",
    "\n",
    "    def render(self, mode):\n",
    "        return self.obs['pov']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# experiment setup\n",
    "wrapped_env = DiscreteMinecraftEnvWrapper(nav_env)\n",
    "random_agent = RandomAgent(wrapped_env.action_space.actions)\n",
    "# default_agent = DefaultAgent(wrapped_env.action_space.actions, 0)\n",
    "experiment = Experiment(wrapped_env, random_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the experiment for 500 steps, visualize every 20 steps\n",
    "experiment.run(500, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5:** For the purpose of this tutorial, we have simplified the observation and action spaces, and the reward definition of the original MineRL-NavDense task. Examine the implementation of `DiscreteMinecraftEnvWrapper`. What observation space and action space are implemented? How do you expect these to simplify the learning task? Consider the implemented reward function. What behavior do you expect an agent to learn based on this reward signal? How do you expect it to differ from behavior learned to optimize the original task reward?\n",
    "\n",
    "Take note of the results you have obtained above. The RandomAgent implementation does not typically exceed a reward of zero on this task. Below, you will train a DQN agent to perform more successfully. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when done using the MineRL experiment, close it down - this will stop the Minecraft client\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent Implementation\n",
    "\n",
    "We are ready to implement our reinforcement learning agent. The code below implements the DQN agent by [Mnih et al. 2015](https://www.nature.com/articles/nature14236/), but instead of a convolutional network we will use 2 fully connected layers (to allow running experiments in reasonable time without GPU).\n",
    "\n",
    "The QLearningAgent class lays out the required components: model network, target network, explorer, replay memory, and optimizer. The components are implemented in turn below.\n",
    "\n",
    "**Note:** key parts of the agent implementation are left open - see exercises below.\n",
    "\n",
    "### QLearningAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(Agent):\n",
    "    \"\"\"Q-Learning agent with function approximation.\"\"\"\n",
    "\n",
    "    def __init__(self, actions, obs_size, **kwargs):\n",
    "        super(QLearningAgent, self).__init__(actions)\n",
    "\n",
    "        self.obs_size = obs_size\n",
    "        self.tau = kwargs.get('tau', .0001)\n",
    "        \n",
    "        self.model_network = QNetwork(self.obs_size, self.num_actions, kwargs.get('nhidden', 512))\n",
    "        self.target_network = QNetwork(self.obs_size, self.num_actions, kwargs.get('nhidden', 512))\n",
    "        self.target_network.copyparams(self.model_network)\n",
    "\n",
    "        self.explorer = EpsilonGreedyExplorer(kwargs.get('epsilon', .1), self.num_actions, self.model_network)\n",
    "\n",
    "        self.memory = ReplayMemory(self.obs_size, kwargs.get('mem_size', 100))\n",
    "        self.optimizer = self.init_optimizer(self.model_network, kwargs.get('learning_rate', .01))\n",
    "\n",
    "        self.gamma = kwargs.get('gamma', .99)\n",
    "        self.minibatch_size = kwargs.get('minibatch_size', 32)\n",
    "        self.epoch_length = kwargs.get('epoch_length', 100)\n",
    "        \n",
    "        self.step_counter = 0\n",
    "        self.current_loss = .0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(self, obs, reward, done, info):\n",
    "\n",
    "    if self.step_counter > 0:\n",
    "        self.memory.observe(self.prev_obs, self.prev_action, reward, done)\n",
    "\n",
    "    action = self.explorer.next_action(\n",
    "                Variable(obs.reshape(1, obs.shape[0])))\n",
    "\n",
    "    # start training after 1 epoch\n",
    "    if self.step_counter > self.epoch_length:\n",
    "        self.current_loss = self.update_model()\n",
    "\n",
    "    self.step_counter += 1\n",
    "    self.prev_action = action\n",
    "    self.prev_obs = obs\n",
    "\n",
    "    # decay epsilon after each epoch\n",
    "    if self.step_counter % self.epoch_length == 0:\n",
    "        self.explorer.epsilon = max(0.05, self.explorer.epsilon * .95)\n",
    "\n",
    "    return action\n",
    "\n",
    "QLearningAgent.step = step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyExplorer(object):\n",
    "    \"\"\"Implements an epsilon greedy exploration policy\"\"\"\n",
    "    \n",
    "    def __init__(self, epsilon, num_actions, model):\n",
    "        self.epsilon = epsilon\n",
    "        self.num_actions = num_actions\n",
    "        self.model = model\n",
    "\n",
    "    def next_action(self, state):\n",
    "\n",
    "        # TODO: implement epsilon-greedy exploration - see Exercise 6\n",
    "\n",
    "        return action_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(Chain):\n",
    "    \"\"\"The neural network architecture as a Chainer Chain - here: single hidden layer\"\"\"\n",
    "\n",
    "    def __init__(self, obs_size, num_actions, nhidden):\n",
    "        \"\"\"Initialize weights\"\"\"\n",
    "        # use LeCunUniform weight initialization for weights\n",
    "        self.initializer = initializers.LeCunUniform()\n",
    "        self.bias_initializer = initializers.Uniform(1e-4)\n",
    "\n",
    "        super(QNetwork, self).__init__(\n",
    "            feature_layer = L.Linear(obs_size, nhidden,\n",
    "                                initialW = self.initializer,\n",
    "                                initial_bias = self.bias_initializer),\n",
    "            action_values = L.Linear(nhidden, num_actions, \n",
    "                                initialW=self.initializer,\n",
    "                                initial_bias = self.bias_initializer)\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"implements forward pass\"\"\"\n",
    "        h = F.relu(self.feature_layer(x))\n",
    "        return self.action_values(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(self):\n",
    "    (s, action, reward, s_next, is_terminal) = self.memory.sample_minibatch(self.minibatch_size)\n",
    "\n",
    "    # compute Q targets (max_a' Q_hat(s_next, a'))\n",
    "\n",
    "    # TODO: implement Q-target computation, see Exercise 7\n",
    "\n",
    "    # compute Q(s, action)\n",
    "    Q = self.model_network(s)\n",
    "    Q_subset = F.reshape(F.select_item(Q, action), (self.minibatch_size, 1))\n",
    "\n",
    "    # compute Huber loss\n",
    "    error = y - Q_subset\n",
    "    loss_clipped = abs(error) * (abs(error.data) > 1) + (error**2) * (abs(error.data) <= 1)\n",
    "    loss = F.sum(loss_clipped) / self.minibatch_size\n",
    "\n",
    "    # perform model update\n",
    "    self.model_network.zerograds() ## zero out the accumulated gradients in all network parameters\n",
    "    loss.backward()\n",
    "    self.optimizer.update()\n",
    "    \n",
    "    # target network tracks the model\n",
    "    for dst, src in zip(self.target_network.params(), self.model_network.params()):\n",
    "        dst.data = self.tau * src.data + (1 - self.tau) * dst.data\n",
    "\n",
    "    return loss.data\n",
    "\n",
    "QLearningAgent.update_model = update_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_optimizer(self, model, learning_rate):\n",
    "\n",
    "    # default: SGD - consider alternative optimizers, see exercise below\n",
    "    optimizer = optimizers.SGD(learning_rate)\n",
    "\n",
    "    optimizer.setup(model)\n",
    "    return optimizer\n",
    "\n",
    "QLearningAgent.init_optimizer = init_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \"\"\"Implements basic replay memory\"\"\"\n",
    "\n",
    "    def __init__(self, observation_size, max_size):\n",
    "        self.observation_size = observation_size\n",
    "        self.num_observed = 0\n",
    "        self.max_size = max_size\n",
    "        self.samples = {\n",
    "                 'obs'      : np.zeros(self.max_size * 1 * self.observation_size,\n",
    "                                       dtype=np.float32).reshape(self.max_size, 1, self.observation_size),\n",
    "                 'action'   : np.zeros(self.max_size * 1, dtype=np.int16).reshape(self.max_size, 1),\n",
    "                 'reward'   : np.zeros(self.max_size * 1).reshape(self.max_size, 1),\n",
    "                 'terminal' : np.zeros(self.max_size * 1, dtype=np.int16).reshape(self.max_size, 1),\n",
    "               }\n",
    "\n",
    "    def observe(self, state, action, reward, done):\n",
    "        index = self.num_observed % self.max_size\n",
    "        self.samples['obs'][index, :] = state\n",
    "        self.samples['action'][index, :] = action\n",
    "        self.samples['reward'][index, :] = reward\n",
    "        self.samples['terminal'][index, :] = done\n",
    "        \n",
    "        self.num_observed += 1\n",
    "        \n",
    "    def sample_minibatch(self, minibatch_size):\n",
    "        max_index = min(self.num_observed, self.max_size) - 1\n",
    "        sampled_indices = np.random.randint(max_index, size=minibatch_size)\n",
    "        \n",
    "        s      = Variable(np.asarray(self.samples['obs'][sampled_indices, :], dtype=np.float32))\n",
    "        s_next = Variable(np.asarray(self.samples['obs'][sampled_indices+1, :], dtype=np.float32))\n",
    "\n",
    "        a      = Variable(self.samples['action'][sampled_indices].reshape(minibatch_size))\n",
    "        r      = self.samples['reward'][sampled_indices].reshape((minibatch_size, 1))\n",
    "        done   = self.samples['terminal'][sampled_indices].reshape((minibatch_size, 1))\n",
    "\n",
    "        return (s, a, r, s_next, done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Exercises\n",
    "\n",
    "Before moving on to the first DQN experiment, you need to complete the code above to obtain a working DQN implementation.\n",
    "\n",
    "**Exercise 6:** Complete the implementation of [Exploration](#Exploration). The method `next_action` should implement epsilon greedy exploration, such that an action is sampled uniformly at random with probability `self.epsilon`, and the greedy action is selected otherwise.\n",
    "\n",
    "**Hint:** You can obtain the Q-values for all available actions by calling `Q = self.model(state)`.\n",
    "\n",
    "**Exercise 7:** Complete the implementation of the [Model Update](#Model-Update) by computing the correct Q-value targets.\n",
    "\n",
    "**Hint:** You can obtain the row-wise max for a quantity computed on a minibatch using: `F.max(batch_of_data, axis=1, keepdims=True)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: DQN on SimpleRooms\n",
    "\n",
    "It's time to test your DQN implementation. Are you ready? The experiment on the SimpleRoom task below is a good test case. The task can be learned within less than 5000 steps. If your learning curve stays flat - something is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_env = SimpleRooms()\n",
    "\n",
    "simple_q_agent = QLearningAgent(\n",
    "    simple_env.action_space.actions,\n",
    "    16, # observation size\n",
    "    nhidden = 512,\n",
    "    epsilon = 1.,\n",
    "    mem_size = 10000,\n",
    "    learning_rate = .5,\n",
    "    tau = .001,\n",
    "    minibatch_size = 32,\n",
    "    epoch_length = 100)\n",
    "\n",
    "simple_q_experiment = Experiment(simple_env, simple_q_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run DQN on SimpleRooms - it should exceed .2 reward in less than 5000 steps (render every 10 steps)\n",
    "simple_q_experiment.run(5000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7:** Consider the [Optimizer](#Optimizer) and its settings above. Chainer supports a wide range of optimizers: https://docs.chainer.org/en/stable/reference/optimizers.html. Can you find an optimizer / learning rate that learns more quickly than the default one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: DQN on MineRL Navigation\n",
    "\n",
    "Now we're ready to test our DQN agent on our discretized Minecraft Navigation task. Again, if everything is implemented correctly, reward should go up within less than 3000 training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wrapped_env = DiscreteMinecraftEnvWrapper(nav_env)\n",
    "minerl_q_agent = QLearningAgent(\n",
    "    wrapped_env.action_space.actions,\n",
    "    28, # observation size\n",
    "    nhidden = 512,\n",
    "    epsilon = 1.,\n",
    "    mem_size = 10000,\n",
    "    learning_rate = .5,\n",
    "    tau = .001,\n",
    "    minibatch_size = 32,\n",
    "    epoch_length = 100)\n",
    "\n",
    "minerl_q_experiment = Experiment(wrapped_env, minerl_q_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the experiment for 5000 steps, visualize every 50 steps\n",
    "# If implemented correctly, DQN should learn to exceed a reward of 0 within less than 5000 steps\n",
    "minerl_q_experiment.run(5000, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9:** We have used a simplified version of the MineRL dense navigation task to allow fast learning, using a simplified observation and action space. Another relatively simple MineRL task is [MineRLTreechop-v0](http://minerl.io/docs/environments/index.html#minerltreechop-v0). Implement an environment wrapper for the tree chop task that results in a similarly simplified task. Can you find a variant in which DQN learns to chop a tree within 10 minutes of training time?\n",
    "\n",
    "**Exercise 10:** An effective extension of the DQN algorithm is [Double DQN](https://arxiv.org/abs/1509.06461) by van Hasselt and colleagues (AAAI 2016). Its key idea is to separate Q-value target computation from the estimate of the max action, to problems due to overestimated Q-values. The resulting algorithm is a small modification of the original DQN algorithm. Implement this modification based on the paper, and repeat your experiments in SimpleRooms and MineRL using the updated algorithm. Can you achieve faster learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the environment to shut down Minecraft when you're done\n",
    "nav_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "Congratulations! You have completed this RL tutorial.\n",
    "\n",
    "Hopefully, this tutorial has piqued your interest in Reinforcement learning. Here are a few more resources to help you get started.\n",
    "\n",
    "**The RL Book:** For an in-depth treatment of RL, I highly recommend the Sutton and Barto book, now in its second edition: http://incompleteideas.net/book/the-book-2nd.html\n",
    "\n",
    "**Code:** This tutorial used chainer to implement a DQN agent from scratch. A wide range of RL baseline and state-of-the-art algorithms is implemented in [chainerrl](https://github.com/chainer/chainerrl). Other popular RL implementations include OpenAI's [Spinning Up RL](https://spinningup.openai.com/en/latest/), the ray project's [RLLib](https://ray.readthedocs.io/en/latest/rllib.html), and Google's [Dopamine](https://github.com/google/dopamine).\n",
    "\n",
    "**The MineRL Competition:** if you've enjoyed learning about RL using Minecraft, check out the MineRL competition. Registrations are still open: the first round closes September 22. Participating in the competition is a great way to deepen your RL skills. There are great prizes and travel grants to win as well!\n",
    "- [MineRL Competition](http://minerl.io/competition): competition overview, docs, data\n",
    "- [Starter Kit](https://github.com/minerllabs/quick_start)\n",
    "- [MineRL Competition at AICrowd](https://www.aicrowd.com/challenges/neurips-2019-minerl-competition) - submission site\n",
    "- [ChainerRL MineRL baselines](https://github.com/minerllabs/quick_start/tree/master/chainerrl_baselines)\n",
    "- [Guss et al. 2019: The MineRL Competition on Sample Efficient Reinforcement Learning using Human Priors](https://arxiv.org/abs/1904.10079) - competition whitepaper\n",
    "\n",
    "**Conferences:**\n",
    "For recent research in reinforcement learning, check out the topics discussed at [RLDM](http://rldm.org/) - an interdisciplinary conference on Reinforcement Learning and Decision Making. Other key conferences with a large portion of RL research are [ICML](https://www.icml.cc/), [ICLR](https://iclr.cc) and [NeurIPS](https://neurips.cc/). A popular event in Europe is the European Workshop on Reinforcement Learning [EWRL](https://ewrl.wordpress.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
