{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (14, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with sparse GPs\n",
    "\n",
    "### James Hensman 2017 - 2019\n",
    "\n",
    "This notebook serves as a tutorial: the aim is to understand how GP classification with sparse approximations work in GPflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First, let's have a look at the illustrative dataset from the lecture. Here's how it was generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a sample from a GP\n",
    "\n",
    "# first build the kernel and kernel matrix\n",
    "k = gpflow.kernels.Matern52(1, variance=6.0)\n",
    "X_all = np.linspace(0, 6, 200).reshape(-1, 1)\n",
    "K = k.compute_K_symm(X_all)\n",
    "\n",
    "# sample from a multivariate normal\n",
    "L = np.linalg.cholesky(K)\n",
    "f_all = np.dot(L, np.random.RandomState(6).randn(200, 1))\n",
    "plt.plot(X_all, f_all, 'C0')\n",
    "\n",
    "# squash\n",
    "p_all = np.exp(f_all) / (1 + np.exp(f_all))\n",
    "plt.plot(X_all, p_all, 'C3')\n",
    "\n",
    "# evaluate a small number of points\n",
    "ind = np.random.randint(0, 200, (50,))\n",
    "X = X_all[ind]\n",
    "p = p_all[ind]\n",
    "plt.plot(X, p, 'C3o', ms=6)\n",
    "\n",
    "\n",
    "# bernoulli draws\n",
    "Y = np.where(np.random.rand(50, 1) < p, 1, 0)\n",
    "plt.plot(X, Y, 'C0x', ms=8, mew=2)\n",
    "\n",
    "plt.xlim(0, 6)\n",
    "plt.ylim(-1.5, 2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: the effect of the parameters on classification datasets\n",
    "\n",
    "a) Change the variance parameter in the kernel above (try 100, 0.01). What is the effect on the data X, Y ?\n",
    "\n",
    "b) What is the effect of the lengthscale parameter on X and Y?\n",
    "\n",
    "Now let's build a GPflow model of these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a GPflow model. it assumes that the inducing locations Z are fixed to the data X.\n",
    "# We'll tell the model that the data are Binary (Bernoulli likelihood), and we'll pick a kernel.\n",
    "m = gpflow.models.SVGP(X, Y,\n",
    "                      likelihood=gpflow.likelihoods.Bernoulli(),\n",
    "                      kern=gpflow.kernels.Matern52(1),\n",
    "                      Z=X.copy())\n",
    "m.feature.Z.set_trainable(False)\n",
    "\n",
    "o = gpflow.train.ScipyOptimizer()\n",
    "o.minimize(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_1d(m):\n",
    "    # work out some sensible limits\n",
    "    xmin, xmax = m.X.read_value().min(), m.X.read_value().max()\n",
    "    xmin, xmax = xmin - 0.1 * (xmax - xmin), xmax + 0.1 * (xmax - xmin)\n",
    "    Xtest = np.linspace(xmin, xmax, 200).reshape(-1, 1)\n",
    "    \n",
    "    # bubble fill the predictions\n",
    "    mu, var = m.predict_f(Xtest)\n",
    "    plt.fill_between(Xtest.flatten(),\n",
    "                     (mu + 2 * np.sqrt(var)).flatten(),\n",
    "                     (mu - 2 * np.sqrt(var)).flatten(),\n",
    "                     alpha=0.3, color='C1')\n",
    "    \n",
    "    # plot samples\n",
    "    samples = m.predict_f_samples(Xtest, 10).squeeze()\n",
    "    plt.plot(Xtest, samples.T, 'C1', lw=1)\n",
    "    \n",
    "    # plot p-samples\n",
    "    p = np.exp(samples) / (1. + np.exp(samples))\n",
    "    plt.plot(Xtest, p.T, 'C3', lw=1)\n",
    "\n",
    "    # plot data\n",
    "    plt.plot(m.X.read_value(), m.Y.read_value(), 'C0x', ms=8, mew=2)\n",
    "    \n",
    "    plt.xlim(xmin, xmax)\n",
    "\n",
    "plot_1d(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we have a model of your dataset. Let's poke the model and see if we can understand how it works. \n",
    "\n",
    "### Exercise 2: poke the model\n",
    "\n",
    "a) Print the model. Can you relate the parameters to the variables discussed in the lecture?\n",
    "\n",
    "b) After optimizing the model, has the model managed to estimate the kernel parameters effectively? \n",
    "\n",
    "c) Since we know what the optimal kernel parameters are, let's see how the model works with those. Assign the known kernel parameters to the model and mark them as not for training (`m.kern.foo.set_trainable(False)`). You will have to optimize the model again. Are these parameters better than the estimated ones? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answers here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification in 2D\n",
    "\n",
    "It's straightforward to move the GP classification problem into 2D by simply changing the kernel. We'll do that here, and in addition relax the assumption that X=Z. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a standard 2D dataset.\n",
    "X_banana = np.loadtxt('data/banana_X_train', delimiter=',')\n",
    "Y_banana = np.loadtxt('data/banana_Y_train', delimiter=',').reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's the sparse GP model:\n",
    "m = gpflow.models.SVGP(X_banana, Y_banana,\n",
    "                       kern=gpflow.kernels.Matern52(2),\n",
    "                       likelihood=gpflow.likelihoods.Bernoulli(),\n",
    "                       Z=np.random.rand(8, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and here's a function for plotting the model\n",
    "def plot_2d(m):\n",
    "    sess = m.enquire_session(None)\n",
    "    #plot the inducing point locations:\n",
    "    if hasattr(m, 'Z'):\n",
    "        Z = m.Z.read_value(sess)\n",
    "        plt.plot(Z[:, 0], Z[:, 1], 'C3o', ms=10, label=\"inducing\")\n",
    "    \n",
    "    xmin, ymin = m.X.read_value(sess).min(0)\n",
    "    xmax, ymax = m.X.read_value(sess).max(0)\n",
    "    xmin, xmax = xmin - 0.1 * (xmax - xmin), xmax + 0.1 * (xmax - xmin)\n",
    "    ymin, ymax = ymin - 0.1 * (ymax - ymin), ymax + 0.1 * (ymax - ymin)\n",
    "    \n",
    "    xx, yy = np.mgrid[xmin:xmax:200j, ymin:ymax:200j]\n",
    "    Xtest = np.vstack([xx.flatten(), yy.flatten()]).T\n",
    "    mu, var = m.predict_y(Xtest)\n",
    "    \n",
    "    X, Y = m.X.read_value(sess), m.Y.read_value(sess)\n",
    "    \n",
    "    for i, level in [[0, 0.2], [1, 0.8]]:\n",
    "        plt.plot(X[Y.flatten()==i, 0], X[Y.flatten()==i, 1], 'C{}o'.format(i), ms=8, label='y={}'.format(i))\n",
    "        cs = plt.contour(xx, yy, mu.reshape(*xx.shape), [level], colors='C{}'.format(i), linewidths=3)\n",
    "        cs.collections[0].set_label('p(y={}) = {}'.format(i, level))\n",
    "                         \n",
    "    cs = plt.contour(xx, yy, mu.reshape(*xx.shape), [0.5], colors='C3', linewidths=1)\n",
    "    cs.collections[0].set_label('p(y=1) = 0.5')\n",
    "    \n",
    "    plt.legend(loc=0)\n",
    "    plt.xlim(xmin, xmax)\n",
    "    plt.ylim(ymin, ymax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: understanding the model\n",
    "\n",
    "a) Use the above function to plot the model. What's going on in the plot? \n",
    "\n",
    "b) Okay, optimize the model (perhaps borrow the code from the 1D model above). Plot again. Better? \n",
    "\n",
    "c) How do the lines in the plots correspond to our distribution on functions?\n",
    "\n",
    "d) Investigate the effect of the *number* of the inducing input points. Try the model with 4, 12, 20, 200 rows in Z.\n",
    "Plot each: what happens?\n",
    "\n",
    "e) Investigate the effect of the *locations* of the inducing points. Does it work if you initialize them far away from the model? What does the model do if you refuse to let it adapt Z (`m.Z.set_trainable(False)`)? \n",
    "\n",
    "bonus) Initialize the model with Z = np.zeros((10, 2)). What happens and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answers here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
