{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T15:52:27.824866Z",
     "start_time": "2017-10-12T15:52:27.614941Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression: a tutorial\n",
    "### James Hensman 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Maximum likelihood linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably the most basic model in statistics is linear regression. We're given some data in the form of input-output pairs $(x_i, y_i)$, and we're asked \"what can you tell me about the value of $y$ at a new $x$?\", and as the simplest model we assume a linear relationship between input and output.\n",
    "\n",
    "In python, we stack the pairs into numpy arrays like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T15:56:06.313534Z",
     "start_time": "2017-10-12T15:56:06.212083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f64282db898>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhmAHgMQQ7ACQmP8HPYRoklh6GWwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6426f17710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's make up a very simple dataset\n",
    "x_list = [-2, -1, 0, 1, 2]\n",
    "X = np.array(x_list).reshape(-1, 1)  # note the shape convention\n",
    "# this makes it easier to generalise the code to multiple dimensions\n",
    "y_list = [-3.6, -2.05, -1.15, 0.1, 1.8]\n",
    "Y = np.array(y_list).reshape(-1, 1)  # note the shape convention\n",
    "\n",
    "plt.plot(X, Y, 'kx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear relationship between input and output is described by the function\n",
    "\n",
    "$f(x) = ax + b$,\n",
    "\n",
    "and we want to find the parameters $(a, b)$ in this equation.\n",
    "\n",
    "In maximum likelihood linear regression, they are determined by maximising the likelihood $p(y \\,|\\, a, b)$ under the formulation\n",
    "\n",
    "$y_i = f(x_i) + \\epsilon_i$\n",
    "\n",
    "with\n",
    "\n",
    "$\\epsilon_i \\sim \\mathcal N(0, \\sigma_n^2)$.\n",
    "\n",
    "This leads to the same result as \"least squares regression\", and this is sometimes called \"the line of best fit\".\n",
    "\n",
    "\n",
    "If we were doing \"deep learning\" we would make $f$ a much more complicated function, and then try to find some suitable parameters by gradient descent.\n",
    "In linear regression, though, there's actually a closed form solution!\n",
    "\n",
    "This is facilitated by the following two functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T16:12:22.290647Z",
     "start_time": "2017-10-12T16:12:22.283658Z"
    }
   },
   "outputs": [],
   "source": [
    "def linear_features(X):\n",
    "    \"\"\"\n",
    "    Creates a feature matrix for linear regression\n",
    "    \"\"\"\n",
    "    return np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "\n",
    "def ml_linear_regression(X, Y, features):\n",
    "    \"\"\"\n",
    "    Returns the maximum likelihood parameters for linear regression\n",
    "    \"\"\"\n",
    "    Phi = features(X)\n",
    "    return np.linalg.solve(np.dot(Phi.T, Phi), np.dot(Phi.T, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex. 1. \n",
    "\n",
    "a) Use the above functions to estimate the parameters $(a, b)$. Do they seem sensible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) What is the ML prediction for $f(0.5)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Plot the maximum likelihood straight line through the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Why does `linear_features` add a column of ones to the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) (advanced) What happened to $\\sigma_n^2$? Why doesn't if affect the answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) (advanced) Derive the expression we've used for the ML estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T16:26:42.179921Z",
     "start_time": "2017-10-12T16:26:42.175411Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your answers here. \n",
    "\n",
    "# For help with the notebook, click on the \"Help\" menu, press [F1] or type \"ctrl-m h\". \n",
    "# For help with understanding, ask a PM team member!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw in the lecture that Bayesian linear regression means starting from a prior distribution over functions and moving to a posterior distribution. In this section we'll examine some functions under the prior and posterior distributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draws from the prior\n",
    "\n",
    "Since we have two parameters, we need a two-dimensional prior distribution. Here we use the multivariate normal distribution, which is defined by the mean vector $\\boldsymbol \\mu_0$ and the covariance matrix $\\mathbf \\Sigma_0$.\n",
    "In maths we write\n",
    "\n",
    "$p(\\boldsymbol \\theta) = \\mathcal N(\\boldsymbol \\mu_0,\\, \\mathbf \\Sigma_0)$.\n",
    "\n",
    "A sensible prior distribution is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T16:45:39.848443Z",
     "start_time": "2017-10-12T16:45:39.841525Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu_0 = np.zeros((2, 1))\n",
    "Sigma_0 = np.eye(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T16:44:46.894382Z",
     "start_time": "2017-10-12T16:44:46.879773Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_theta(mu, Sigma):\n",
    "    \"\"\"\n",
    "    For a given distribution, sample a vector for theta\n",
    "    \"\"\"\n",
    "    sample = mu + np.dot(np.linalg.cholesky(Sigma), np.random.randn(mu.size, 1))\n",
    "    return sample\n",
    "\n",
    "def plot_linear_function(theta, lower=-3, upper=3):\n",
    "    \"\"\"\n",
    "    Plot a linear function with parameters theta over the range (lower, upper)\n",
    "    \"\"\"\n",
    "    Xtest = np.linspace(lower, upper, 200).reshape(-1, 1)\n",
    "    Phi = linear_features(Xtest)\n",
    "    f = np.dot(Phi, theta)\n",
    "    plt.plot(Xtest, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex. 2\n",
    "a) Use (maybe modify?) the above helper functions to plot 10 samples from the prior on the same figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Change the prior. Can you interpret what happens by plotting? For example, try `Sigma_0 = np.array([[1., 0], [0, 0.001]])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your answers here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draws from the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like maximum likelihood linear regression, Bayesian linear regression has a closed form. Here's a helper:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T16:31:03.567906Z",
     "start_time": "2017-10-12T16:31:03.553003Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bayesian_linear_regression(X, Y, features, mu_0, Sigma_0, sigma_n):\n",
    "    \"\"\"\n",
    "    Returns the posterior distribution (mu_p, Sigma_p) \n",
    "    of the parameters theta in Bayesian linear regression\n",
    "    \"\"\"\n",
    "    precision = np.linalg.inv(Sigma_0)\n",
    "    Phi = features(X)\n",
    "    \n",
    "    Sigma_p = np.linalg.inv(precision + np.dot(Phi.T, Phi) / sigma_n ** 2)\n",
    "    mu_p = np.dot(Sigma_p, np.dot(precision, mu_0 + np.dot(Phi.T, Y) / sigma_n **2))\n",
    "    \n",
    "    return mu_p, Sigma_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex. 3.\n",
    "a) Improve the docstring in `bayesian_linear_regression` by adding type annotation :p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Compute the posterior distribution for our tiny dataset. What's the relationship between `mu_p` and the maximum-likelihood estimate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Draw and plot 100 functions from the posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) How can we pick a sensible value for $\\sigma_n$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) What would happen if we had more (or fewer) data? Generate your own dataset and find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T15:25:37.786277Z",
     "start_time": "2017-10-02T15:25:37.782056Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your answers here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with non-linear basis functions\n",
    "\n",
    "Linear regression can approximate nonlinear functions using nonlinear basis functions. It's still called linear regression because the relationship between the output and the *_basis_* is linear, even though the relationship between the output and the *input* might not be.\n",
    "How can we use this stuff to learn nonlinear functions? The usual answer is to project the problem into some features space. Don't worry if that sounds hard: we just have to replace the `linear_features` function above, and we'll be rollin'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial Basis Function networks\n",
    "Radial basis function networks is a \"neural network\" that is actually just linear regression on some flexible features. The form of the features is:\n",
    "\n",
    "$\\phi_i(x) = \\exp(-(x - c_i) / \\ell^2)$,\n",
    "\n",
    "where the parameters $c_i$ are so-called 'centers' of the radial basis functions (we will assume them to be located on a regular grid), and $\\ell$ is the width. Here's a simple implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T16:53:31.761291Z",
     "start_time": "2017-10-12T16:53:31.750527Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RBFFeatures(object):\n",
    "    def __init__(self, centers, width):\n",
    "        self.centers, self.width = centers, width\n",
    "    def __call__(self, X):\n",
    "        return np.exp(-np.square((X - self.centers) / self.width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate this model, we're going to need a more interesting dataset. Here's one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T16:53:35.056432Z",
     "start_time": "2017-10-12T16:53:34.954207Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.sort(np.random.rand(100)).reshape(-1,1)\n",
    "Y = np.sin(18 * X) + 2.5 * np.cos(5 * X) + 0.5 * np.random.randn(*X.shape)\n",
    "plt.plot(X, Y, 'kx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a sensible but rather arbitrary set of features and a prior distribution for the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T17:13:25.076645Z",
     "start_time": "2017-10-12T17:13:25.070210Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_centers = 5\n",
    "c = np.linspace(0, 1, N_centers)\n",
    "width = 0.1\n",
    "f = RBFFeatures(c, width)\n",
    "\n",
    "mu_0 = np.zeros((10, N_centers))\n",
    "Sigma_0 = np.eye(N_centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex. 4\n",
    "\n",
    "a) Can you plot a sample from this prior? What happens to the functions away from the centers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Compute the posterior and plot some sample functions. (hint: try `sigma_n = 0.5`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) What happens to the *posterior* functions away from the centers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) What is the effect of using more / fewer basis functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) What is the effect of the width parameter? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) How on earth are we going to set all of these parameters??!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) (advanced) Can the fit be improved by optimizing the location of the basis functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
