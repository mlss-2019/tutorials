{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab session 2: GPs and GP regression with GPflow \n",
    "\n",
    "This tutorial is adapted from the Gaussian Process Summer School that is running at the University of Sheffield since 2013. It also strongly inspired from the gpflow online documentation.\n",
    "\n",
    "The aim of this lab session is to illustrate the concepts seen during the lectures. We will focus on three aspects of GPs: the kernel, the random sample paths and the GP regression model.\n",
    "\n",
    "We assume that GPflow is already installed on your machine and that your are running the 1.0.0 version.\n",
    "\n",
    "We first tell the jupyter notebook that we want the plots to appear inline, then we import the libraries we will need:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gpflow\n",
    "assert gpflow.__version__ == '1.0.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.   Getting started: The Covariance Function\n",
    "\n",
    "There are many covariance functions (i.e. kernels) that are already implemented in gpflow, such as \n",
    "\n",
    " - GPflow.kernels.Constant\n",
    "\n",
    " - GPflow.kernels.Linear\n",
    "\n",
    " - GPflow.kernels.White\n",
    "\n",
    " - GPflow.kernels.Matern12\n",
    "\n",
    " - GPflow.kernels.Matern32\n",
    "\n",
    " - GPflow.kernels.Matern52\n",
    "\n",
    " - GPflow.kernels.RBF\n",
    "\n",
    " - GPflow.kernels.Cosine\n",
    "\n",
    " - GPflow.kernels.PeriodicKernel\n",
    "\n",
    "\n",
    "Let's start with a Matern 5/2 kernel in one dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = gpflow.kernels.Matern52(input_dim=1, variance=1., lengthscales=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary of the kernel can be obtained using the command `print(k)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Parameter name:\u001b[1mMatern52/variance\u001b[0m [trainable] prior:None shape:() transform:+ve>\n",
      "value: 1.0\n",
      "\n",
      "<Parameter name:\u001b[1mMatern52/lengthscales\u001b[0m [trainable] prior:None shape:() transform:+ve>\n",
      "value: 0.2\n",
      "1.0\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "print(k)\n",
    "\n",
    "## If you are only interested in one parameter value, you can also do\n",
    "print(k.variance.read_value())\n",
    "print(k.lengthscales.read_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a kernel $k(.,.)$ is a function of two inputs (plus some parameters), it is common to freeze one of the variable to a given value (say $0.5$) and to plot the kernel as a function of the remaining one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_grid = np.linspace(0, 1, 100).reshape(100,1)     # get 100 points evenly spaced shaped as a column vector\n",
    "plt.plot(x_grid, k.compute_K(x_grid, np.array([[.5]])))\n",
    "plt.ylim((0,1.2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "\n",
    "a) Here is how to change the `variance` and `lengthscales` kernel parameters. What are their influence on the covariance function? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.variance = 0.5\n",
    "k.lengthscales = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Since the kernel corresponds to the covariance between the GP values:\n",
    "$$ k(x,y) = cov(Y(x), Y(y))$$\n",
    "What should that imply for the GP samples?\n",
    "\n",
    "c) plot three other covariance functions, do they all look like \"hat functions\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.   Sampling from a Gaussian Process\n",
    "\n",
    "By definition, the values taken by a GP evaluated on a grid follow a multivariate normal distribution. Given the mean function and the kernel we can thus evaluate them on a grid and use a multivariate normal sampler to  plot some samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = gpflow.kernels.Matern52(input_dim=1, variance=1., lengthscales=0.2)\n",
    "\n",
    "mu = 0 * x_grid.flatten()            # vector of the means, it is just zero here.\n",
    "C = k.compute_K(x_grid, x_grid)      # compute the covariance matrix associated with inputs x_grid\n",
    "\n",
    "# Generate 20 separate samples paths from a Gaussian with mean mu and covariance C\n",
    "Z = np.random.multivariate_normal(mu,C,20)\n",
    "\n",
    "# plot them all\n",
    "plt.plot(x_grid, Z.T);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "a) modify the above code to change the kernel and the mean function. What is the effect on the samples? Can you find a kernel such that the samples are continuous but not differentiable? One the other hand, can you find a kernel such that the samples are extremely smooth?\n",
    "\n",
    "b) Investigate the influence of the kernel parameters on the samples. For example, try a lengthscale of 0.01, and a lengthscale of 10. Are the effects in line with your answer from the previous exercise? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  A Gaussian Process Regression Model\n",
    "\n",
    "We will now combine the Gaussian process prior with some data to form a GP regression model with GPflow. We will generate data from the function $f ( x ) = − \\cos(\\pi x ) + \\sin(4\\pi x )$ over $[0, 1]$, adding some noise to give $y(x) = f(x) + \\epsilon$, with the noise being Gaussian distributed, $\\epsilon \\sim \\mathcal{N}(0, 0.01)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f547eab4a90>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrHsJ8sAbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f547eaf3630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test_function(x):\n",
    "    y = -np.cos(np.pi*x) + np.sin(4*np.pi*x) + np.random.normal(loc=0.0, scale=0.1, size=x.shape) \n",
    "    return(4*y)\n",
    "\n",
    "X = np.linspace(0.05, 0.95, 10).reshape(10, 1)\n",
    "Y = test_function(X)\n",
    "\n",
    "plt.plot(X, Y, 'kx', mew=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to reconstruct an approximation of the test function using only this data, and the prior knowledge that the function is very smooth (i.e. infinitely differentiable).\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "a) What kernel would you suggest in order to account for this prior knowledge? \n",
    "\n",
    "b) Similarly, can you hand pick reasonable values for the `variance` and `lengthscales` for this problem? \n",
    "\n",
    "c) Update the lines below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = gpflow.kernels.Matern32(input_dim=1, variance=1., lengthscales=.2)\n",
    "k.lengthscales = 1.\n",
    "k.variance = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a GP regression model is then quite simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = gpflow.models.GPR(X, Y, k)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot our model predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x_grid, m):\n",
    "    mean_pred, var_pred = m.predict_f(x_grid)\n",
    "    upper_95 = mean_pred + 2*np.sqrt(var_pred)\n",
    "    lower_95 = mean_pred - 2*np.sqrt(var_pred)\n",
    "\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.plot(x_grid, mean_pred, color='#204a87')\n",
    "    plt.fill_between(x_grid[:,0], lower_95[:,0], upper_95[:,0], color='#204a87', alpha=.2)\n",
    "    plt.plot(m.X.read_value(), m.Y.read_value(), 'kx', mew=1.5);\n",
    "\n",
    "plot(x_grid, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction variance is quite wrong on this example! The next exercise is about setting it right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "*a)* By default, the noise variance is set to 1 when the model is created. You can modify this by assigning a new value by running `m.likelihood.variance = 0.1`. Can you tweak this parameter to obtain a more sensible model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*b)* What happen to the model predition if you change the kernel paramerers (you can directly assign values to `m.kern.variance` and `m.kern.lengthscales`)? Is that in accordance with your answers from Ex 2.b? \n",
    "\n",
    "As we have seen previously, generating samples from a GP requires the vector corresponding to the mean function and the covariance matrix. The first one is already computed as `mean_pred`, but `var_pred` is only a vector of the variance values and not the full covariance matrix. \n",
    "\n",
    "*c)* Use the `m.predict_f_full_cov` function to obtain the full covariance matrix. Generate and plot samples from the posterior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Covariance Function Parameter Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen during the course, the parameters values can be estimated by maximizing the likelihood of the observations. Since we don’t want one of the variance to become negative during the optimization, the parameteres are by default constrained to be positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can optimize the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpflow.train.ScipyOptimizer().minimize(m)\n",
    "\n",
    "## print m\n",
    "print(m.kern.variance.read_value())\n",
    "print(m.kern.lengthscales.read_value())\n",
    "print(m.likelihood.variance.read_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x_grid, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Computing the mean value of the test function\n",
    "\n",
    "In this section, we will see that using the GP model leads to more accurate predictions than if we where to use only the raw data. We will focus here on one particular quantity to predict: the average value of the test function (i.e. its ingregral). Since this is a toy example, calculus can show that this average value is exactly 0. \n",
    "\n",
    "Monte-Carlo is the cannonical method for approximating average values. It consists in taking the average value of a very large number of data points with inputs distributed uniformly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(0, 1, (1000,1))\n",
    "Y = test_function(X)\n",
    "\n",
    "plt.plot(X.T, Y.T, 'kx')\n",
    "np.mean(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the result is approximated pretty well but we had to use 1000 datapoints! In practice, it is often the case that the number of data points is limited. Let's see if we can retreive this value with only 20 observations of the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(0, 1, (20,1))\n",
    "Y = test_function(X)\n",
    "\n",
    "plt.plot(X.T, Y.T, 'kx')\n",
    "np.mean(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of the approximation is definitely not as good as before. On the other hand, we can use those 10  points to build a GP regression model and then approximate the average value of the test function by the average value of the posterior mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = gpflow.kernels.Matern32(input_dim=1, variance=1., lengthscales=.2)\n",
    "\n",
    "m = gpflow.models.GPR(X, Y, k)\n",
    "m.compile()\n",
    "\n",
    "gpflow.train.ScipyOptimizer().minimize(m)\n",
    "\n",
    "plot(x_grid, m)\n",
    "\n",
    "mean_pred, var_pred = m.predict_f(x_grid)\n",
    "np.mean(mean_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "*a)* Which of the two methods based on 20 data points seems to return a value closest to the target? Can you tell why?\n",
    "\n",
    "*b)* For now we have only used the mean prediction and not the uncertainty that is also provided by the model. In order to properly quantify the uncertainty, we can generate samples from the conditional distribution and then compute the average value of the samples. This average value will thus be random since it is based on a random sample paths and it is possible to compute its mean and variance. Can you write the code that does so?\n",
    "\n",
    "*c)* (advanced) In this case, the distribution of the average values of the samples can be computed analytically using the linearity of the expectation and the covariance. Give it a shot if you want to!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further exercises: Combining Covariance Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In GPflow you can easily combine covariance functions you have created using the sum and product operators, `+` and `*`. So, for example, if we wish to combine an exponentiated quadratic covariance with a Matern 5/2 then we can write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Parameter name:\u001b[1mAdd/rbf/variance\u001b[0m [trainable] prior:None shape:() transform:+ve>\n",
      "value: 1.0\n",
      "\n",
      "<Parameter name:\u001b[1mAdd/rbf/lengthscales\u001b[0m [trainable] prior:None shape:() transform:+ve>\n",
      "value: 2.0\n",
      "\n",
      "<Parameter name:\u001b[1mAdd/matern52/variance\u001b[0m [trainable] prior:None shape:() transform:+ve>\n",
      "value: 2.0\n",
      "\n",
      "<Parameter name:\u001b[1mAdd/matern52/lengthscales\u001b[0m [trainable] prior:None shape:() transform:+ve>\n",
      "value: 4.0\n"
     ]
    }
   ],
   "source": [
    "kern1 = gpflow.kernels.RBF(1, variance=1., lengthscales=2.)\n",
    "kern2 = gpflow.kernels.Matern52(1, variance=2., lengthscales=4.)\n",
    "kern = kern1 + kern2\n",
    "\n",
    "print(kern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if we wanted to multiply them we can write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Parameter name:\u001b[1mProd/rbf/variance\u001b[0m [trainable] prior:None shape:() transform:+ve>\n",
      "value: 1.0\n",
      "\n",
      "<Parameter name:\u001b[1mProd/rbf/lengthscales\u001b[0m [trainable] prior:None shape:() transform:+ve>\n",
      "value: 2.0\n",
      "\n",
      "<Parameter name:\u001b[1mProd/matern52/variance\u001b[0m [trainable] prior:None shape:() transform:+ve>\n",
      "value: 2.0\n",
      "\n",
      "<Parameter name:\u001b[1mProd/matern52/lengthscales\u001b[0m [trainable] prior:None shape:() transform:+ve>\n",
      "value: 4.0\n"
     ]
    }
   ],
   "source": [
    "kern = kern1 * kern2\n",
    "\n",
    "print(kern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "a) Generate samples from the above kernels. \n",
    "\n",
    "b) Can you build a kernel such that the samples look similar to the following plots?\n",
    "\n",
    "![samples to replicate](img/mystery_samples.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
